# ACyLeR: Enhanced iTransformer for Time-Series Forecasting

## iTransformer Architecture
### Self-Attention Mechanism
### Positional Encoding
### Feed-Forward Networks
### Layer Normalization
### Residual Connections

## Adaptive Cycling Learning Rate (ACLR)
### Cycling Learning Rate
### Adaptive Adjustment
### Dynamic Rate Control
### Improved Convergence
### Reduced Oscillations

## Long-Term Time-Series Forecasting
### Challenges of Long Sequences
### iTransformer Suitability
### ACyLeR Enhancement
### Forecasting Accuracy
### Temporal Dependencies

## Experimental Evaluation
### Datasets Used
### Baseline Models
### Performance Metrics (RMSE, MAE)
### Comparative Analysis
### Statistical Significance

## Key Contributions
### Novel ACLR Algorithm
### Enhanced iTransformer Model
### Improved Forecasting Performance
### Demonstrated on Multiple Datasets
### Open-Source Implementation
